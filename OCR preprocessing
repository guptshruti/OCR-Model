import cv2
from craft_text_detector import Craft
from ultralytics import YOLO
import pytesseract
from PIL import Image
import numpy as np

# Path to your input document image
image_path = "document_image.jpg"

# Initialize CRAFT detector for text regions
craft = Craft(output_dir='output', crop_type="word", cuda=False)  # Set cuda=True if you have GPU

# Load the pre-trained YOLOv8 model for detecting images/tables
yolo_model = YOLO('yolov8x.pt')  # You can replace this with a custom-trained model if needed

# Step 1: Detect Text using CRAFT
print("Running CRAFT for text detection...")
image = cv2.imread(image_path)
prediction_result = craft.detect_text(image)

# Extract word bounding boxes from CRAFT
text_boxes = prediction_result['boxes']
print(f"Detected text regions: {len(text_boxes)}")

# Step 2: Detect other objects (images, tables) using YOLOv8
print("Running YOLOv8 for image and table detection...")
yolo_results = yolo_model.predict(image)

# Extract bounding boxes for tables/images
yolo_boxes = []
for result in yolo_results:
    for box in result.boxes:
        yolo_boxes.append(box.xyxy)  # Add detected object coordinates
print(f"Detected non-text objects (tables, images, etc.): {len(yolo_boxes)}")

# Step 3: Process Text using OCR
def perform_ocr(image, box):
    """ Perform OCR on a cropped region defined by a bounding box. """
    # Crop the region of interest
    x_min, y_min, x_max, y_max = box
    cropped_image = image[int(y_min):int(y_max), int(x_min):int(x_max)]
    
    # Use pytesseract or replace this with your own OCR model pipeline
    extracted_text = pytesseract.image_to_string(cropped_image, lang='eng')
    return extracted_text

# Initialize the list to store extracted text
ocr_results = []

# Process each text region detected by CRAFT
print("Extracting text using OCR...")
for box in text_boxes:
    ocr_text = perform_ocr(image, box)
    ocr_results.append({'box': box, 'text': ocr_text})
    
# Step 4: Process detected non-text regions (images, tables) if necessary
# For each detected region by YOLO (e.g., tables or images), you can crop and process accordingly
print("Processing detected images and tables...")
for box in yolo_boxes:
    x_min, y_min, x_max, y_max = box
    cropped_image = image[int(y_min):int(y_max), int(x_min):int(x_max)]
    
    # Optionally save cropped images of detected objects
    cv2.imwrite(f'output/cropped_object_{x_min}_{y_min}.jpg', cropped_image)

# Step 5: Visualize and save results (Optional)
print("Saving results...")
for i, ocr_item in enumerate(ocr_results):
    box = ocr_item['box']
    x_min, y_min, x_max, y_max = box
    # Draw bounding box on the image for visualization
    cv2.rectangle(image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)
    # Annotate the extracted text on the image
    cv2.putText(image, ocr_item['text'], (int(x_min), int(y_min)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Save the final annotated image
cv2.imwrite("output/annotated_image.jpg", image)

# Clean up CRAFT resources
craft.unload_craftnet_model()
craft.unload_refinenet_model()

print("Process completed.")
